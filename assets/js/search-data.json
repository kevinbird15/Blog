{
  
    
        "post0": {
            "title": "Decouple_WD Parameter Deep Dive",
            "content": "Summary . If you don&#39;t plan to use weight decay (wd=0), you can safely ignore this parameter. . If you do plan to use weight decay, but don&#39;t have any idea whether to decouple weight decay or not, use the default of True. It works the same as l2_reg on sgd and doesn&#39;t have . I don&#39;t currently know when you would want to use l2_reg vs weight decay unless trying to recreate historical results. . If wd == 0: don&#39;t worry about decouple_wd | If decouple_wd is True, weight_decay is used, otherwise, l2_reg is used | weight_decay updates the value of params, l2_reg updates the param&#39;s gradients | weight_decay is directly tied to learning rate, l2_reg is not | Further Reading | . Deep Dive . This is the first blog post in my No Second Turkey series. The concept that will be explored today is hyperparameters for SGD. Let&#39;s just dive right in: . Initially, my plan was to explore hyperparameters for SGD, but during the first hyperparameter that I looked at, I was derailed. In this case, I felt like the exploration for this one parameter was interesting enough to have its own post. We will explore more parameters and hyperparameters in the future, but for now, I am going to publish a deep dive on just decouple_wd. . from fastai.vision.all import * . show_doc has been created to replace ?? so anywhere that you see show_doc, you can do ??func_name instead. That version doesn&#39;t show up in the blog very well so I&#39;ve modified it with the help of Zach Mueller . def show_doc_with_source(func_name): print(get_ipython().inspector._get_info(func_name, detail_level=1)[&#39;text/plain&#39;]) . show_doc_with_source(show_doc_with_source) . Signature: (func_name) Docstring: &lt;no docstring&gt; Source: def show_doc_with_source(func_name): print(get_ipython().inspector._get_info(func_name, detail_level=1)[&#39;text/plain&#39;]) File: ~/Research/optimizers/&lt;ipython-input-2-f848c98c9c35&gt; Type: function . I am going to dig into decouple_wd. It looks like it defaults to True, but what is that actually changing? First, let&#39;s look at the sgd code: . Look at SGD definition . show_doc_with_source(SGD) . Signature: (params, lr, mom=0.0, wd=0.0, decouple_wd=True) Source: @log_args(to_return=True, but_as=Optimizer.__init__) def SGD(params, lr, mom=0., wd=0., decouple_wd=True): &#34;A `Optimizer` for SGD with `lr` and `mom` and `params`&#34; cbs = [weight_decay] if decouple_wd else [l2_reg] if mom != 0: cbs.append(average_grad) cbs.append(sgd_step if mom==0 else momentum_step) return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd) File: ~/development/fastai/fastai/optimizer.py Type: function . Answers lead to more questions. It looks like decouple_wd determines if weight_decay or l2_reg should be added to the callback list. My initial hunch is that these are two different forms of weight decay, but let&#39;s dig into each of them and see how they both work. . cbs = [weight_decay] if decouple_wd else [l2_reg] . show_doc_with_source(weight_decay) . Signature: (p, lr, wd, do_wd=True, **kwargs) Source: def weight_decay(p, lr, wd, do_wd=True, **kwargs): &#34;Weight decay as decaying `p` with `lr*wd`&#34; if do_wd and wd!=0: p.data.mul_(1 - lr*wd) File: ~/development/fastai/fastai/optimizer.py Type: function . show_doc_with_source(l2_reg) . Signature: (p, lr, wd, do_wd=True, **kwargs) Source: def l2_reg(p, lr, wd, do_wd=True, **kwargs): &#34;L2 regularization as adding `wd*p` to `p.grad`&#34; if do_wd and wd!=0: p.grad.data.add_(p.data, alpha=wd) File: ~/development/fastai/fastai/optimizer.py Type: function . Looking at the arguments each of these take in, it looks like they take in the exact same information. So it is really saying do I want to do thing 1 with these variables, or do I want to do thing 2? . First, I&#39;m going to discect weight decay: . weight_decay . Part 1: if do_wd and wd!=0: This first part is already super important for our problem. Since we have wd set to 0 by default, neither of these functions will do anything in the beginning. That kind of answers my initial question of whether decouple_wd should be True or False, but let&#39;s finish exploring this branch while we are going down it. . Part 2: p.data.mul_(1 - lr*wd) This is a nice clean line of code, but what does it actually mean/do? . p represents our params (went and verified in optimizer code). . This is a Tensor and I can tell that because of how p.data is used. That is usually a giveaway that the parameter is having its gradients calculated. (You&#39;ll get a big error if you try to work with p directly instead of p.data) . mul_ is saying multiply something and store it back into p.data. I&#39;m going to look at up for sure mostly because I want to see what type of multiplication is used.. The underscore runs the same function, but in pytorch, that is shorthand that says to store the results back into the tensor (similar to inplace=True in pandas if you are more familiar with that library). . 1 - lr*wd So when wd = 0, this would just multiple the params by 1 so there would be no affect which is also probably why it is skipped in that case. Let&#39;s think about what happens in the alternative case though. In the case where wd = 0.1 and lr = 1, this constant would be 0.9 so it would take every weight and cut it down. So in practice, what does this do for us? Our model can still tell the model to step back in the same direction. I may have to think about this a bit more to understand why we would want to use weight decay. For now, I am going to move on to l2_reg. . L2_reg . L2_reg also checks if do_wd and wd!=0 similar to how weight decay works. This is a bypass valve that says don&#39;t waste your calculations if wd is 0 or if you decide that you don&#39;t want any weight decay. . p.grad.data is a lot different than p.data from weight_decay. p.grad.data is actually looking at the gradient that was calculated. . add_ is then used to add a number into the gradients. . (p.data, alpha=wd) . I&#39;m going to analyze this third part slightly out of order. First I want to know what alpha=wd means: Torch&#39;s add function has an argument called alpha. Alpha is a scalar that will be multiplied with the first argument. In our case, p.data. . Our model weights get multiplied by a scalar value and that value is added to the gradient. Let&#39;s try to figure out what this is trying to accomplish. . weight_decay dampens down the model&#39;s params directly 1-lr*wd amount. My initial thoughts with this is that I don&#39;t like that lr is involved in this process. I&#39;m not sure if that is an important piece that I&#39;m not understanding, but it is something that I would rather not have involved in my weight_decay parameter. . l2_reg adds a small value associated with the weights directly to the gradient. This isn&#39;t intuitive to me. I&#39;m curious why adding a value that is scaled with a small value wd would be helpful for our model. If anything, it seems like it would just end up adding a small amount of noise. This one doesn&#39;t use the learning rate which I like. . Since I don&#39;t feel like I currently have a good understanding of either weight_decay or l2_reg, I&#39;m going to go do some searching and see if I can find some intuition around these two concepts. . I&#39;m going to pull in the tst_param function which allows using a specific value and giving a specific gradient for the tensor. . Exploring single param . #shamelessly stolen from fastai docs: https://docs.fast.ai/optimizer#sgd_step def tst_param(val, grad=None): &quot;Create a tensor with `val` and a gradient of `grad` for testing&quot; res = tensor([val]).float() res.grad = tensor([val/10 if grad is None else grad]).float() return res . p = tst_param(1, 0.2) p_before = deepcopy(p.data) pg_before = deepcopy(p.grad.data) weight_decay(p, 1., 0.1) p_after = deepcopy(p.data) pg_after = deepcopy(p.grad.data) print(&#39;Before and After p: &#39;, p_before, &#39;-&gt;&#39;, p_after) print(&#39;Before and After pg: &#39;, pg_before, &#39;-&gt;&#39;, pg_after) . Before and After p: tensor([1.]) -&gt; tensor([0.9000]) Before and After pg: tensor([0.2000]) -&gt; tensor([0.2000]) . I was going to cycle through some numbers, but instead I&#39;m going to cycle through a bunch and plot the output of p and p.grad . learning_rate_l = [0.001, 0.01, 0.1, 1] weight_decay_l = [0.001, 0.01, 0.1, 1] . learning_rate_grid, weight_decay_grid = np.meshgrid(learning_rate_l, weight_decay_l) . learning_rate_grid . array([[0.001, 0.01 , 0.1 , 1. ], [0.001, 0.01 , 0.1 , 1. ], [0.001, 0.01 , 0.1 , 1. ], [0.001, 0.01 , 0.1 , 1. ]]) . weight_decay_grid . array([[0.001, 0.001, 0.001, 0.001], [0.01 , 0.01 , 0.01 , 0.01 ], [0.1 , 0.1 , 0.1 , 0.1 ], [1. , 1. , 1. , 1. ]]) . for lr_list,wd_list in zip(learning_rate_grid, weight_decay_grid): ; . p_val = [] pg_val = [] for lr,wd in zip(lr_list, wd_list): p = tst_param(1, 0.2) weight_decay(p, lr, wd) p_val.append(p.data) pg_val.append(p.grad.data) print(&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.data) print(&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.grad.data) . lr: 0.001 wd: 1.0 - tensor([0.9990]) lr: 0.001 wd: 1.0 - tensor([0.2000]) lr: 0.01 wd: 1.0 - tensor([0.9900]) lr: 0.01 wd: 1.0 - tensor([0.2000]) lr: 0.1 wd: 1.0 - tensor([0.9000]) lr: 0.1 wd: 1.0 - tensor([0.2000]) lr: 1.0 wd: 1.0 - tensor([0.]) lr: 1.0 wd: 1.0 - tensor([0.2000]) . p_val . [tensor([0.9990]), tensor([0.9900]), tensor([0.9000]), tensor([0.])] . pg_val . [tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000])] . Weight_Decay param viewing . p_val = [] pg_val = [] for lr_list,wd_list in zip(learning_rate_grid, weight_decay_grid): for lr,wd in zip(lr_list, wd_list): p = tst_param(1, 0.2) weight_decay(p, lr, wd) p_val.append(p.data) pg_val.append(p.grad.data) print(&quot;param| &quot;,&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.data) print(&quot;grad | &quot;,&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.grad.data) . param| lr: 0.001 wd: 0.001 - tensor([1.0000]) grad | lr: 0.001 wd: 0.001 - tensor([0.2000]) param| lr: 0.01 wd: 0.001 - tensor([1.0000]) grad | lr: 0.01 wd: 0.001 - tensor([0.2000]) param| lr: 0.1 wd: 0.001 - tensor([0.9999]) grad | lr: 0.1 wd: 0.001 - tensor([0.2000]) param| lr: 1.0 wd: 0.001 - tensor([0.9990]) grad | lr: 1.0 wd: 0.001 - tensor([0.2000]) param| lr: 0.001 wd: 0.01 - tensor([1.0000]) grad | lr: 0.001 wd: 0.01 - tensor([0.2000]) param| lr: 0.01 wd: 0.01 - tensor([0.9999]) grad | lr: 0.01 wd: 0.01 - tensor([0.2000]) param| lr: 0.1 wd: 0.01 - tensor([0.9990]) grad | lr: 0.1 wd: 0.01 - tensor([0.2000]) param| lr: 1.0 wd: 0.01 - tensor([0.9900]) grad | lr: 1.0 wd: 0.01 - tensor([0.2000]) param| lr: 0.001 wd: 0.1 - tensor([0.9999]) grad | lr: 0.001 wd: 0.1 - tensor([0.2000]) param| lr: 0.01 wd: 0.1 - tensor([0.9990]) grad | lr: 0.01 wd: 0.1 - tensor([0.2000]) param| lr: 0.1 wd: 0.1 - tensor([0.9900]) grad | lr: 0.1 wd: 0.1 - tensor([0.2000]) param| lr: 1.0 wd: 0.1 - tensor([0.9000]) grad | lr: 1.0 wd: 0.1 - tensor([0.2000]) param| lr: 0.001 wd: 1.0 - tensor([0.9990]) grad | lr: 0.001 wd: 1.0 - tensor([0.2000]) param| lr: 0.01 wd: 1.0 - tensor([0.9900]) grad | lr: 0.01 wd: 1.0 - tensor([0.2000]) param| lr: 0.1 wd: 1.0 - tensor([0.9000]) grad | lr: 0.1 wd: 1.0 - tensor([0.2000]) param| lr: 1.0 wd: 1.0 - tensor([0.]) grad | lr: 1.0 wd: 1.0 - tensor([0.2000]) . p_val = [] pg_val = [] for lr_list,wd_list in zip(learning_rate_grid, weight_decay_grid): for lr,wd in zip(lr_list, wd_list): p = tst_param(1, 0.2) p_before = deepcopy(p.data) pg_before = deepcopy(p.grad.data) weight_decay(p, lr, wd) p_after = deepcopy(p.data) pg_after = deepcopy(p.grad.data) p_val.append(p.data) pg_val.append(p.grad.data) print(&quot;param| &quot;,&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p_before, &#39;-&gt;&#39;, p_after) #print(&quot;grad | &quot;,&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, pg_before, &#39;-&gt;&#39;, pg_after) . param| lr: 0.001 wd: 0.001 - tensor([1.]) -&gt; tensor([1.0000]) param| lr: 0.01 wd: 0.001 - tensor([1.]) -&gt; tensor([1.0000]) param| lr: 0.1 wd: 0.001 - tensor([1.]) -&gt; tensor([0.9999]) param| lr: 1.0 wd: 0.001 - tensor([1.]) -&gt; tensor([0.9990]) param| lr: 0.001 wd: 0.01 - tensor([1.]) -&gt; tensor([1.0000]) param| lr: 0.01 wd: 0.01 - tensor([1.]) -&gt; tensor([0.9999]) param| lr: 0.1 wd: 0.01 - tensor([1.]) -&gt; tensor([0.9990]) param| lr: 1.0 wd: 0.01 - tensor([1.]) -&gt; tensor([0.9900]) param| lr: 0.001 wd: 0.1 - tensor([1.]) -&gt; tensor([0.9999]) param| lr: 0.01 wd: 0.1 - tensor([1.]) -&gt; tensor([0.9990]) param| lr: 0.1 wd: 0.1 - tensor([1.]) -&gt; tensor([0.9900]) param| lr: 1.0 wd: 0.1 - tensor([1.]) -&gt; tensor([0.9000]) param| lr: 0.001 wd: 1.0 - tensor([1.]) -&gt; tensor([0.9990]) param| lr: 0.01 wd: 1.0 - tensor([1.]) -&gt; tensor([0.9900]) param| lr: 0.1 wd: 1.0 - tensor([1.]) -&gt; tensor([0.9000]) param| lr: 1.0 wd: 1.0 - tensor([1.]) -&gt; tensor([0.]) . #param updates for weight_decay plt.plot(p_val) plt.show() . #gradient doesn&#39;t update for weight_decay plt.plot(pg_val) plt.show() . show_doc_with_source(weight_decay) . Signature: (p, lr, wd, do_wd=True, **kwargs) Source: def weight_decay(p, lr, wd, do_wd=True, **kwargs): &#34;Weight decay as decaying `p` with `lr*wd`&#34; if do_wd and wd!=0: p.data.mul_(1 - lr*wd) File: ~/development/fastai/fastai/optimizer.py Type: function . p_val . [tensor([1.0000]), tensor([1.0000]), tensor([0.9999]), tensor([0.9990]), tensor([1.0000]), tensor([0.9999]), tensor([0.9990]), tensor([0.9900]), tensor([0.9999]), tensor([0.9990]), tensor([0.9900]), tensor([0.9000]), tensor([0.9990]), tensor([0.9900]), tensor([0.9000]), tensor([0.])] . pg_val . [tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000]), tensor([0.2000])] . Key Takeaways . The gradient doesn&#39;t change when doing weight decay | LR and WD are tied to each other lr*wd | . May be worth investigating adaptive learning rates with weight decay. May end up with more weight decay than intended. . Let&#39;s repeat that process for l2_reg . show_doc_with_source(l2_reg) . Signature: (p, lr, wd, do_wd=True, **kwargs) Source: def l2_reg(p, lr, wd, do_wd=True, **kwargs): &#34;L2 regularization as adding `wd*p` to `p.grad`&#34; if do_wd and wd!=0: p.grad.data.add_(p.data, alpha=wd) File: ~/development/fastai/fastai/optimizer.py Type: function . l2_reg takes in lr as a parameter, but it doesn&#39;t actually do anything with it. So my guess is that it only has it as a parameter to keep it consistent with weight decay. . p_val = [] pg_val = [] for lr_list,wd_list in zip(learning_rate_grid, weight_decay_grid): for lr,wd in zip(lr_list, wd_list): p = tst_param(1, 0.2) l2_reg(p, lr, wd) p_val.append(p.data) pg_val.append(p.grad.data) print(&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.data) print(&quot;lr:&quot;,lr,&quot;wd:&quot;,wd, &quot;-&quot;, p.grad.data) . lr: 0.001 wd: 0.001 - tensor([1.]) lr: 0.001 wd: 0.001 - tensor([0.2010]) lr: 0.01 wd: 0.001 - tensor([1.]) lr: 0.01 wd: 0.001 - tensor([0.2010]) lr: 0.1 wd: 0.001 - tensor([1.]) lr: 0.1 wd: 0.001 - tensor([0.2010]) lr: 1.0 wd: 0.001 - tensor([1.]) lr: 1.0 wd: 0.001 - tensor([0.2010]) lr: 0.001 wd: 0.01 - tensor([1.]) lr: 0.001 wd: 0.01 - tensor([0.2100]) lr: 0.01 wd: 0.01 - tensor([1.]) lr: 0.01 wd: 0.01 - tensor([0.2100]) lr: 0.1 wd: 0.01 - tensor([1.]) lr: 0.1 wd: 0.01 - tensor([0.2100]) lr: 1.0 wd: 0.01 - tensor([1.]) lr: 1.0 wd: 0.01 - tensor([0.2100]) lr: 0.001 wd: 0.1 - tensor([1.]) lr: 0.001 wd: 0.1 - tensor([0.3000]) lr: 0.01 wd: 0.1 - tensor([1.]) lr: 0.01 wd: 0.1 - tensor([0.3000]) lr: 0.1 wd: 0.1 - tensor([1.]) lr: 0.1 wd: 0.1 - tensor([0.3000]) lr: 1.0 wd: 0.1 - tensor([1.]) lr: 1.0 wd: 0.1 - tensor([0.3000]) lr: 0.001 wd: 1.0 - tensor([1.]) lr: 0.001 wd: 1.0 - tensor([1.2000]) lr: 0.01 wd: 1.0 - tensor([1.]) lr: 0.01 wd: 1.0 - tensor([1.2000]) lr: 0.1 wd: 1.0 - tensor([1.]) lr: 0.1 wd: 1.0 - tensor([1.2000]) lr: 1.0 wd: 1.0 - tensor([1.]) lr: 1.0 wd: 1.0 - tensor([1.2000]) . plt.plot(p_val) plt.show() . plt.plot(pg_val) plt.show() . Key Takeaways l2_reg . The gradient is updated using wd*param (higher param values would have a higher weight decay penalty | param values remain unchanged when using l2_reg | . If I&#39;m understanding the difference between l2_reg and weight_decay, l2_reg directly modifies the param gradient&#39;s by using a hyper parameter wd and the param values themselves. . Whereas weight decay also has a wd parameter, but it uses that parameter alongside the lr to create a dampener on the weights. So the higher the learning rate, the more weight_decay will dampen the params and same with wd. . Further Reading . Here are a few links to explore from members of the fastai community: . https://blog.janestreet.com/l2-regularization-and-batch-norm/ (ChrisK) | https://arxiv.org/abs/1711.05101 (ilovescience) | https://en.wikipedia.org/wiki/Tikhonov_regularization (ilovescience) | .",
            "url": "https://impartialderivative.com/no%20second%20turkey/2020/09/05/Decouple_WD_Parameter_Deep_Dive.html",
            "relUrl": "/no%20second%20turkey/2020/09/05/Decouple_WD_Parameter_Deep_Dive.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "The No Second Turkey Learning Concept",
            "content": "When I reflect on learning processes that work for me, writing out my thought process comes pretty high on the list for me. A second thing is to re-explain the concept in my own words. Blog posts that I tag as “no second turkey” are going to be as much as possible, unaltered sessions of my thought process. In the past, everything I end up sharing is a polished version of my code and the truth of the matter is, there are a lot of ugly steps along the way that never get published which can be really intimidating if you don’t see the steps of the journey. . In cooking shows, the host has a perfect looking turkey ready to pull out of the oven which is why I’m calling this the ‘no second turkey’ approach. If things don’t go well, we may end up with a burnt turkey or two. But we will learn something along the way and hopefully it will be more focused on coding and ideas around fastai rather than cooking! . I know some readers aren’t going to care about the process, but may still want to know what I learned during my journey. For this, I will start each blog with an executive summary section that will summarize anything interesting that I find along the way. These won’t include any code, but may link to sections in the notebook that will provide more color. This goal of this section is to be simple enough that anybody could understand the takeaways. .",
            "url": "https://impartialderivative.com/no%20second%20turkey/2020/09/04/The-No-Second-Turkey-Learning-Concept.html",
            "relUrl": "/no%20second%20turkey/2020/09/04/The-No-Second-Turkey-Learning-Concept.html",
            "date": " • Sep 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "High Level Haha Architecture",
            "content": "Hiromi (Medium and Twitter) and I recently competed in the Haha 2019 Humor Detection challenge and received a second place finish on classification (predicting whether this tweet was intended to be humor by the author) and a third place finish on regression (predicting an average funniness score value for a tweet in a 5-star ranking). . This post is written by both Hiromi and I and will contain a high-level explanation of our approach. Hopefully, this is something that people find value in and if there are any parts that aren’t clear, please reach out to either of us on Twitter or the Fastai forums and we will clarify our approach! . Data Preprocessing . First, a tokenizer was created using SentencePiece which would chop the tweets into small word-fragments called tokens. The SentencePiece model was trained on a set of 500k tweets downloaded from Twitter using Tweepy. SentencePiece allows users to fine-tune its underlying model and adjust to better tokenize the corpus provided. The SentencePiece tokenizer was then injected into the Fastai Data Block API. The Data Block API orchestrates the preprocessing, organizing, and grouping the data so it is in the form (cleverly named “databunch”) appropriate to be fed into a neural network . By preprocessing, we mean tokenizing and numericalizing each token because neural networks take numbers as inputs, not text/tokens. To numericalize tokens, we needed a vocabulary which maps tokens to numbers. Our vocab consists of approximately 30k tokens that appeared in the Haha 2019 corpus. After running our data through the Data Block API, we had a training set with 90% of our data, a validation set with the remaining 10%, and corresponding data loaders. . Language Model Generation . In order to give the model a better starting point, we trained a language model. A language model predicts what the next word would be given preceding few words. The purpose is not to create something that can generate a sentence, but rather to gain a basic understanding of the language itself. This information is stored in the word embeddings and the model’s weights and biases which can then be transferred to the classifier and regressor to give them a much better base for the problems they are meant to solve. . Classifier Architecture . We ensembled five different models for our classification task: . A forward AWD-LSTM using fastai’s ULMFiT | A backward AWD-LSTM | A pre-trained BERT model (bert-base-multilingual-cased) based on this repo. | Another BERT pretrained model (bert-base-multilingual-uncased) where everything is lowercase. | Naïve Bayes - Support Vector Machine (NBSVM). This was a technique that was borrowed from Jeremy Howard’s Kaggle kernel and his fantastic lecture. | Regressor Architecture . Our regressor was made up of the first four models from above. The only difference was that the output of the final linear layer is just one number instead of two - a number indicating the funniness score. We initially fed in only the tweets the classifier determined to be humorous into the regressor. The intuition here was not bombard the regressor with not funny tweets (which was 61.4% the training data) and really hone in on the score itself. After experimenting, our initial hypothesis turned out to be false. The regressor trained much better with not-funny tweets included. . Generating Final Results . The output of the classifier tells the percent likelihood of a tweet being humorous. The competition’s sample submission file indicated to submit the binary entry of whether it is funny (1) or not (0). The straightforward way to do this is to set the threshold to 50% and anything below it becomes 0; 1 otherwise. We instead examined the validation set predictions to determine the threshold that would maximize the F1-score which turned out to be lower than 0.5 (~0.45). For our final run, we used the entire dataset for training so we kept the predetermined threshold. Below is a graph of what we based our decision on: . . Another tweak we did was to take into account the output of the regressor for the classification prediction. If the regressor determined a funniness score of less than 1, we marked the tweet to be not funny (i.e. 0). Looking back, we feel that there was room for improvement here. Perhaps we could have taken into account the classifier output a little bit more by adding yet another threshold. Finally, we clipped the regressor predictions so that they fell between 0 and 5. . Conclusion . This competition was a wonderful experience and a great learning opportunity. The fastai community has always supported us when we are stuck, pushed us to do more, and gave us a deeper understanding of the materials. . A few things that stuck out to us during this competition were: . Ensembling of the output of many different models is important. | Change just one thing at a time and don’t make any assumptions. | Using pre-trained language model is becoming much easier thanks to fastai, ULMFit, and BERT. | Acknowledgments . We would like to thank Jeremy Howard, Rachel Thomas, and the entire Fast.ai team for providing a world-class education to many aspiring ML practitioner around the world. .",
            "url": "https://impartialderivative.com/2019/06/26/High-Level-Haha-Architecture.html",
            "relUrl": "/2019/06/26/High-Level-Haha-Architecture.html",
            "date": " • Jun 26, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Haha 2019 Lessons Learned",
            "content": "I recently competed in the Haha 2019 Competition and ended up getting second place on predicting whether a tweet would be funny or not and third place on predicting how funny the tweet would be. . . I wanted to share some of my lessons learned from this competition to hopefully prevent somebody else (maybe even my future self) from making the same mistakes. . Using git add . to put my code into a Github Repo . git add . is the only way that I have ever added files to a project and I learned that it is a terrible way to add files to a Github repo. Basically this says to add every file in your current directory recursively into your Github repo. I’m sure if you know what you are doing, this isn’t an issue, but I did not. This was my attempt to put a starting repo into Github and then I was going to figure out what I actually wanted up there and take the rest down. What ended up happening instead was I added all of the garbage files and every big file into my repo. A better approach would have been to start small and only add the files that I definitely wanted added. This for me would have been the .ipynb files and not a whole lot else. . | Creating a new databunch every time I reran the notebook . One mistake that I made that became more obvious as the competition got closer to completion was creating a new databunch every time I ran the notebook. The problem with doing this is that not only does it waste time, it also changes up the validation set every time. This is great when you are wanting to make sure that your model is generalizable, but it is less ideal when trying to test whether a change is actually helpful and your score goes up slightly when looking at your validation scores. Saving the databunch will help you maintain your sanity when trying slight tweaks and improvements. . | Not saving things required for reproducibility . This ended up being one of my less impactful issues, but it could have been a lot bigger deal. I didn’t realize that not only did I need to save the model to reload the model, I also needed to save the vocab associated with the databunch. I put it lower than the databunch mistake because if I would have done that, it would have made this less important. . #Load vocab in fastai sp_vocab = Vocab.load(&quot;30k_Haha.vocab&quot;) #Save vocab in fastai sp_vocab.save(&quot;30k_Haha.vocab&quot;) . | Not keeping the Github Repo clean . This may seem similar to #1 and it is similar, but it is a different issue. Where did my submissions go? Where did my notebooks live? Where did my test notebooks go? All of these were in my root directory. This isn’t an issue when you have one notebook and one dataset, but it will grow. Just set your structure up at the beginning and have a directory for test notebooks, solid notebooks, data, models, and submissions. . | Not Auto-dating Everything that saves . When I saved things, I manually changed the names of everything. This led to some files being overwritten that I would have liked to have had. I also would have used this in #4 to not only put them in the directories I mentioned, but also add a date file to those folders so I could have more organization. . I would do something like this next time and run it in the first cell of the notebook so that all of the cells from that run would be timestamped together. . import datetime year, month, day, hour, minute, second,_,_,_ = datetime.datetime.now().timetuple() . | Not Trying Blah . One quote that I first heard from Jeremy Howard during his fastai deep learning course is that if you are wondering if blah is a good idea, to try blah and see. Until you test out an idea it could be both the best and worst idea you’ve ever had and you won’t know which it is until you test it out. . | Not changing hyper-parameters . It took me way too long to start modifying the hyper-parameters when working on this problem. Change everything and see how it helps or hurts things! . | Thanks for reading and hopefully there is something useful here for you. My idea is to do a recap like this after each competition and hopefully over time it makes me a better coder and practitioner. My next blog post is going to be a walkthrough of Hiromi and my approach to this competition because even with all of the mistakes I made, we ended up with a pretty solid classifier and regressor at the end of the day. .",
            "url": "https://impartialderivative.com/2019/06/11/Haha-2019-Lessons-Learned.html",
            "relUrl": "/2019/06/11/Haha-2019-Lessons-Learned.html",
            "date": " • Jun 11, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Impartial Derivative is a blog designed to look at technical concepts and ideas with an impartial eye. My name is Kevin Bird and I have a passion for innovation, machine learning, and automation. I received my bachelor’s degree from the University of Nebraska in Omaha studying Computer Engineering. I also studied entrepreneurship while I was in school and am excited to discuss new ideas and also technologically challenging ideas. Helping others solve challenges in implementations and execution of ideas are two things that I enjoy. . Since graduating from college, I have focused on learning a lot of the concepts that I passed tests on in college, but didn’t fully understand. My goal when I am learning new concepts post-college is to understand the intuition behind the concepts and to be able to explain the concept in my own words. That is where my blog comes in. I will document my learning journey and do my best to convey the concepts in simple terms. I also want to preserve as much of the learning process as well. My goal when writing blog posts is to include a summary to allow anybody to get the key takeaways even if the article goes a little deeper than an individual cares about. . I have continued to hone my skills using fast.ai, Kaggle, and other competitions. I have had a few finishes that I am happy about including a 2nd place finish in the HackerEarth Predict the Happiness Competition, a 2nd place finish in the Haha 2019 Funniness Prediction Classification challenge, and a 3rd place finish in the Haha 2019 Funniness Prediction Regression challenge. I also have three top 20% finishes on Kaggle. (Dog Breed Identification, Spooky Author Identification, and Quick, Draw! Doodle Recognition Challenge) . Productionalizing models by creating APIs in Flask has been one of the areas that I enjoy focusing on. One of the areas I am most excited to use this functionality is combining automation with predictive models to allow a bot to tackle more complex problems that may not be easy this-or-that decisions. .",
          "url": "https://impartialderivative.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "",
          "content": "",
          "url": "https://impartialderivative.com/_pages/",
          "relUrl": "/_pages/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://impartialderivative.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}