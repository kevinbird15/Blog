{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://impartialderivative.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "High Level Haha Architecture",
            "content": "Hiromi (Medium and Twitter) and I recently competed in the Haha 2019 Humor Detection challenge and received a second place finish on classification (predicting whether this tweet was intended to be humor by the author) and a third place finish on regression (predicting an average funniness score value for a tweet in a 5-star ranking). . This post is written by both Hiromi and I and will contain a high-level explanation of our approach. Hopefully, this is something that people find value in and if there are any parts that aren’t clear, please reach out to either of us on Twitter or the Fastai forums and we will clarify our approach! . Data Preprocessing . First, a tokenizer was created using SentencePiece which would chop the tweets into small word-fragments called tokens. The SentencePiece model was trained on a set of 500k tweets downloaded from Twitter using Tweepy. SentencePiece allows users to fine-tune its underlying model and adjust to better tokenize the corpus provided. The SentencePiece tokenizer was then injected into the Fastai Data Block API. The Data Block API orchestrates the preprocessing, organizing, and grouping the data so it is in the form (cleverly named “databunch”) appropriate to be fed into a neural network . By preprocessing, we mean tokenizing and numericalizing each token because neural networks take numbers as inputs, not text/tokens. To numericalize tokens, we needed a vocabulary which maps tokens to numbers. Our vocab consists of approximately 30k tokens that appeared in the Haha 2019 corpus. After running our data through the Data Block API, we had a training set with 90% of our data, a validation set with the remaining 10%, and corresponding data loaders. . Language Model Generation . In order to give the model a better starting point, we trained a language model. A language model predicts what the next word would be given preceding few words. The purpose is not to create something that can generate a sentence, but rather to gain a basic understanding of the language itself. This information is stored in the word embeddings and the model’s weights and biases which can then be transferred to the classifier and regressor to give them a much better base for the problems they are meant to solve. . Classifier Architecture . We ensembled five different models for our classification task: . A forward AWD-LSTM using fastai’s ULMFiT | A backward AWD-LSTM | A pre-trained BERT model (bert-base-multilingual-cased) based on this repo. | Another BERT pretrained model (bert-base-multilingual-uncased) where everything is lowercase. | Naïve Bayes - Support Vector Machine (NBSVM). This was a technique that was borrowed from Jeremy Howard’s Kaggle kernel and his fantastic lecture. | Regressor Architecture . Our regressor was made up of the first four models from above. The only difference was that the output of the final linear layer is just one number instead of two - a number indicating the funniness score. We initially fed in only the tweets the classifier determined to be humorous into the regressor. The intuition here was not bombard the regressor with not funny tweets (which was 61.4% the training data) and really hone in on the score itself. After experimenting, our initial hypothesis turned out to be false. The regressor trained much better with not-funny tweets included. . Generating Final Results . The output of the classifier tells the percent likelihood of a tweet being humorous. The competition’s sample submission file indicated to submit the binary entry of whether it is funny (1) or not (0). The straightforward way to do this is to set the threshold to 50% and anything below it becomes 0; 1 otherwise. We instead examined the validation set predictions to determine the threshold that would maximize the F1-score which turned out to be lower than 0.5 (~0.45). For our final run, we used the entire dataset for training so we kept the predetermined threshold. Below is a graph of what we based our decision on: . . Another tweak we did was to take into account the output of the regressor for the classification prediction. If the regressor determined a funniness score of less than 1, we marked the tweet to be not funny (i.e. 0). Looking back, we feel that there was room for improvement here. Perhaps we could have taken into account the classifier output a little bit more by adding yet another threshold. Finally, we clipped the regressor predictions so that they fell between 0 and 5. . Conclusion . This competition was a wonderful experience and a great learning opportunity. The fastai community has always supported us when we are stuck, pushed us to do more, and gave us a deeper understanding of the materials. . A few things that stuck out to us during this competition were: . Ensembling of the output of many different models is important. | Change just one thing at a time and don’t make any assumptions. | Using pre-trained language model is becoming much easier thanks to fastai, ULMFit, and BERT. | Acknowledgments . We would like to thank Jeremy Howard, Rachel Thomas, and the entire Fast.ai team for providing a world-class education to many aspiring ML practitioner around the world. .",
            "url": "https://impartialderivative.com/2019/06/26/High-Level-Haha-Architecture.html",
            "relUrl": "/2019/06/26/High-Level-Haha-Architecture.html",
            "date": " • Jun 26, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Haha 2019 Lessons Learned",
            "content": "I recently competed in the Haha 2019 Competition and ended up getting second place on predicting whether a tweet would be funny or not and third place on predicting how funny the tweet would be. . . I wanted to share some of my lessons learned from this competition to hopefully prevent somebody else (maybe even my future self) from making the same mistakes. . Using git add . to put my code into a Github Repo . git add . is the only way that I have ever added files to a project and I learned that it is a terrible way to add files to a Github repo. Basically this says to add every file in your current directory recursively into your Github repo. I’m sure if you know what you are doing, this isn’t an issue, but I did not. This was my attempt to put a starting repo into Github and then I was going to figure out what I actually wanted up there and take the rest down. What ended up happening instead was I added all of the garbage files and every big file into my repo. A better approach would have been to start small and only add the files that I definitely wanted added. This for me would have been the .ipynb files and not a whole lot else. . | Creating a new databunch every time I reran the notebook . One mistake that I made that became more obvious as the competition got closer to completion was creating a new databunch every time I ran the notebook. The problem with doing this is that not only does it waste time, it also changes up the validation set every time. This is great when you are wanting to make sure that your model is generalizable, but it is less ideal when trying to test whether a change is actually helpful and your score goes up slightly when looking at your validation scores. Saving the databunch will help you maintain your sanity when trying slight tweaks and improvements. . | Not saving things required for reproducibility . This ended up being one of my less impactful issues, but it could have been a lot bigger deal. I didn’t realize that not only did I need to save the model to reload the model, I also needed to save the vocab associated with the databunch. I put it lower than the databunch mistake because if I would have done that, it would have made this less important. . #Load vocab in fastai sp_vocab = Vocab.load(&quot;30k_Haha.vocab&quot;) #Save vocab in fastai sp_vocab.save(&quot;30k_Haha.vocab&quot;) . | Not keeping the Github Repo clean . This may seem similar to #1 and it is similar, but it is a different issue. Where did my submissions go? Where did my notebooks live? Where did my test notebooks go? All of these were in my root directory. This isn’t an issue when you have one notebook and one dataset, but it will grow. Just set your structure up at the beginning and have a directory for test notebooks, solid notebooks, data, models, and submissions. . | Not Auto-dating Everything that saves . When I saved things, I manually changed the names of everything. This led to some files being overwritten that I would have liked to have had. I also would have used this in #4 to not only put them in the directories I mentioned, but also add a date file to those folders so I could have more organization. . I would do something like this next time and run it in the first cell of the notebook so that all of the cells from that run would be timestamped together. . import datetime year, month, day, hour, minute, second,_,_,_ = datetime.datetime.now().timetuple() . | Not Trying Blah . One quote that I first heard from Jeremy Howard during his fastai deep learning course is that if you are wondering if blah is a good idea, to try blah and see. Until you test out an idea it could be both the best and worst idea you’ve ever had and you won’t know which it is until you test it out. . | Not changing hyper-parameters . It took me way too long to start modifying the hyper-parameters when working on this problem. Change everything and see how it helps or hurts things! . | Thanks for reading and hopefully there is something useful here for you. My idea is to do a recap like this after each competition and hopefully over time it makes me a better coder and practitioner. My next blog post is going to be a walkthrough of Hiromi and my approach to this competition because even with all of the mistakes I made, we ended up with a pretty solid classifier and regressor at the end of the day. .",
            "url": "https://impartialderivative.com/2019/06/11/Haha-2019-Lessons-Learned.html",
            "relUrl": "/2019/06/11/Haha-2019-Lessons-Learned.html",
            "date": " • Jun 11, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "My name is Kevin Bird and I have a passion for innovation, machine learning, and automation. . I went to the University of Nebraska in Omaha, NE where I received my bachelors degree in Computer Engineering. . While I was there, I worked as an Intern at Conagra for 2 years. I helped automate some server fixes based off health checks that were being filtered into everybody’s email folders. This is also where I learned how to create shell scripts and automate processes. During my time at Conagra, I was also exposed to data analytics for the first time. I wasn’t doing data analytics myself at this time, but it was at this point where I realized that I wanted to explore this area further. I had a great experience at Conagra, but I knew I needed to try a smaller company. . At this point, I transitioned to a startup, Flywheel, where I learned a lot about web development and general environment and version control. I used some of my server experience I learned at Conagra to help analyze logs to detect malicious activity and create a dashboard to show the support team what our customers were saying about us. . I had a short stint at Sogeti as a consultant where I learned the things I did and didn’t like about being a consultant. I really like the utility that consultants can provide, but shouldn’t be the only person on a project. At the end of the day, I believe that a company needs to have employees that own the process and take ownership of the process. . I started at Werner Enterprises as a data analyst where my role was to look at data, find insights, and communicate them to the business. I was introduced to an ETL tool called Alteryx and have been using it since. During this time at Werner, I helped build out a process that allowed the workflows being built in Alteryx to use event-based scheduling instead of requiring a certain time to be set. . After working at Werner for about a year and a half, I learned about a project to bring Robotic Process Automation to Werner and I jumped on the opportunity to get back into Automation. I helped start the RPA team at Werner with a POC where I worked close with business. I have grown with this team and have done a lot of business process automation. This involved working closely with business users to understand their manual processes well enough to automate the process. . Over the past few years, I have been learning how to analyze data using online . I have continued to hone my skills using fast.ai, Kaggle, and other competitions. I have had a few finishes that I am happy about including a 2nd place finish in the HackerEarth Predict the Happiness Competition, a 2nd place finish in the Haha 2019 Funniness Prediction Classification challenge, and a 3rd place finish in the Haha 2019 Funniness Prediction Regression challenge. I also have three top 20% finishes on Kaggle (Dog Breed Identification, Spooky Author Identification, and Quick, Draw! Doodle Recognition Challenge) . More recently, I have been focusing on putting our models into APIs so they can be called by systems that benefit from using the predictions our team is able to output. This enables us to make any of our systems able to use our algorithms and really opens up the possibilities. One of the areas I am most excited to use this functionality is combining the RPA work I’ve done with some of these predictive models to allow a bot to tackle more complex problems that may not be easy this or that decisions. . .",
          "url": "https://impartialderivative.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "",
          "url": "https://impartialderivative.com/_pages/",
          "relUrl": "/_pages/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://impartialderivative.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}