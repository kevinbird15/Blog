
<p>Hiromi (<a href="https://medium.com/@hiromi_suenaga">Medium</a> and <a href="https://twitter.com/hiromi_suenaga">Twitter</a>) and I recently competed in the<a href="https://www.fing.edu.uy/inco/grupos/pln/haha/"> Haha 2019 Humor Detection challenge</a> and received a second place finish on classification (predicting whether this tweet was intended to be humor by the author) and a third place finish on regression (predicting an average funniness score value for a tweet in a 5-star ranking).</p>

<p>This post is written by both Hiromi and I and will contain a high-level explanation of our approach. Hopefully, this is something that people find value in and if there are any parts that aren’t clear, please reach out to either of us on Twitter or the Fastai forums and we will clarify our approach!</p>

<h4 id="data-preprocessing">Data Preprocessing</h4>

<p>First, a tokenizer was created using <a href="https://github.com/google/sentencepiece">SentencePiece</a> which would chop the tweets into small word-fragments called tokens. The SentencePiece model was trained on a set of 500k tweets downloaded from Twitter using <a href="https://www.tweepy.org/">Tweepy</a>. SentencePiece allows users to fine-tune its underlying model and adjust to better tokenize the corpus provided. The SentencePiece tokenizer was then injected into the <a href="https://docs.fast.ai/data_block.html">Fastai Data Block API</a>. The Data Block API orchestrates the preprocessing, organizing, and grouping the data so it is in the form (cleverly named “databunch”) appropriate to be fed into a neural network . By preprocessing, we mean <a href="https://docs.fast.ai/text.data.html#TokenizeProcessor">tokenizing</a> and <a href="https://docs.fast.ai/text.data.html#NumericalizeProcessor">numericalizing</a> each token because neural networks take numbers as inputs, not text/tokens. To numericalize tokens, we needed a vocabulary which maps tokens to numbers. Our vocab consists of approximately 30k tokens that appeared in the Haha 2019 corpus. After running our data through the Data Block API, we had a training set with 90% of our data, a validation set with the remaining 10%, and corresponding data loaders.</p>

<h4 id="language-model-generation">Language Model Generation</h4>

<p>In order to give the model a better starting point, we trained a language model. A language model predicts what the next word would be given preceding few words. The purpose is not to create something that can generate a sentence, but rather to gain a basic understanding of the language itself. This information is stored in the word embeddings and the model’s weights and biases which can then be transferred to the classifier and regressor to give them a much better base for the problems they are meant to solve.</p>

<h4 id="classifier-architecture">Classifier Architecture</h4>

<p>We ensembled five different models for our classification task:</p>

<ol>
  <li>A forward <a href="https://docs.fast.ai/text.models.html#AWD_LSTM">AWD-LSTM</a> using fastai’s <a href="https://arxiv.org/pdf/1801.06146.pdf">ULMFiT</a></li>
  <li>A backward AWD-LSTM</li>
  <li>A pre-trained BERT model (bert-base-multilingual-cased) based on <a href="https://github.com/huggingface/pytorch-pretrained-BERT">this repo</a>.</li>
  <li>Another BERT pretrained model (bert-base-multilingual-uncased) where everything is lowercase.</li>
  <li>Naïve Bayes - Support Vector Machine (NBSVM). This was a technique that was borrowed from <a href="https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline">Jeremy Howard’s Kaggle kernel</a> and his fantastic <a href="https://www.youtube.com/watch?v=37sFIak42Sc&amp;feature=youtu.be&amp;t=3745">lecture</a>.</li>
</ol>

<h4 id="regressor-architecture">Regressor Architecture</h4>

<p>Our regressor was made up of the first four models from above. The only difference was that the output of the final linear layer is just one number instead of two - a number indicating the funniness score. We initially fed in only the tweets the classifier determined to be humorous into the regressor. The intuition here was not bombard the regressor with not funny tweets (which was 61.4% the training data) and really hone in on the score itself. After experimenting, our initial hypothesis turned out to be false. The regressor trained much better with not-funny tweets included.</p>

<h4 id="generating-final-results">Generating Final Results</h4>

<p>The output of the classifier tells the percent likelihood of a tweet being humorous. The competition’s sample submission file indicated to submit the binary entry of whether it is funny (1) or not (0). The straightforward way to do this is to set the threshold to 50% and anything below it becomes 0; 1 otherwise. We instead examined the validation set predictions to determine the threshold that would maximize the F1-score which turned out to be lower than 0.5 (~0.45). For our final run, we used the entire dataset for training so we kept the predetermined threshold. Below is a graph of what we based our decision on:</p>

<p><img src="\assets\images\2019-06-26-High-Level-Haha-Architecture\Threshold_Finder.png" alt="Threshold_Finder" /></p>

<p>Another tweak we did was to take into account the output of the regressor for the classification prediction. If the regressor determined a funniness score of less than 1, we marked the tweet to be not funny (i.e. 0). Looking back, we feel that there was room for improvement here. Perhaps we could have taken into account the classifier output a little bit more by adding yet another threshold. Finally, we clipped the regressor predictions so that they fell between 0 and 5.</p>

<h4 id="conclusion">Conclusion</h4>

<p>This competition was a wonderful experience and a great learning opportunity. The fastai community has always supported us when we are stuck, pushed us to do more, and gave us a deeper understanding of the materials.</p>

<p>A few things that stuck out to us during this competition were:</p>

<ol>
  <li>Ensembling of the output of many different models is important.</li>
  <li>Change just one thing at a time and don’t make any assumptions.</li>
  <li>Using pre-trained language model is becoming much easier thanks to <a href="https://www.fast.ai/">fastai</a>, ULMFit, and BERT.</li>
</ol>

<h4 id="acknowledgments">Acknowledgments</h4>

<p>We would like to thank <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a>, <a href="https://twitter.com/math_rachel">Rachel Thomas</a>, and the entire Fast.ai team for providing a world-class education to many aspiring ML practitioner around the world.</p>
